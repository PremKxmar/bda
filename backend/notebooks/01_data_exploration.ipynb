{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4cab01",
   "metadata": {},
   "source": [
    "# ðŸ™ï¸ NYC Taxi Data Exploration\n",
    "\n",
    "## Smart City Real-Time Traffic Simulation & Prediction\n",
    "\n",
    "This notebook explores the NYC Yellow Taxi trip dataset to understand:\n",
    "- Data structure and schema\n",
    "- Data quality issues\n",
    "- Geographic distribution\n",
    "- Temporal patterns\n",
    "- Statistics for ML feature engineering\n",
    "\n",
    "**Dataset**: ~7 GB of NYC Yellow Taxi trips (2015-2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436de980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Data directory - where your CSV files are located\n",
    "DATA_DIR = Path(r\"c:\\sem6-real\\bigdata\\vscode\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ“ Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27338014",
   "metadata": {},
   "source": [
    "## 1. Discover Dataset Files\n",
    "\n",
    "Let's find all the taxi data files and check their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc475a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all taxi CSV files\n",
    "taxi_files = list(DATA_DIR.glob('yellow_tripdata_*.csv'))\n",
    "\n",
    "print(f\"ðŸ“Š Found {len(taxi_files)} taxi data files:\\n\")\n",
    "\n",
    "total_size = 0\n",
    "for f in taxi_files:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    size_gb = size_mb / 1024\n",
    "    total_size += size_mb\n",
    "    print(f\"  ðŸ“„ {f.name}: {size_mb:.2f} MB ({size_gb:.2f} GB)\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ðŸ“¦ TOTAL SIZE: {total_size:.2f} MB ({total_size/1024:.2f} GB)\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4937e8",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data\n",
    "\n",
    "We'll load the first 100,000 rows to understand the data schema without loading the entire 7GB dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample from the first file\n",
    "sample_file = taxi_files[0]\n",
    "print(f\"ðŸ“‚ Loading sample from: {sample_file.name}\")\n",
    "\n",
    "# Read first 100,000 rows\n",
    "df_sample = pd.read_csv(sample_file, nrows=100000)\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(df_sample):,} rows\")\n",
    "print(f\"ðŸ“Š Columns: {len(df_sample.columns)}\")\n",
    "print(f\"\\nðŸ·ï¸ Column names:\")\n",
    "for i, col in enumerate(df_sample.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3878ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "print(\"ðŸ” First 5 rows of the dataset:\\n\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e35dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and memory usage\n",
    "print(\"ðŸ“‹ Data Types and Info:\\n\")\n",
    "df_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14178ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"ðŸ“ˆ Statistical Summary:\\n\")\n",
    "df_sample.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b09a8f",
   "metadata": {},
   "source": [
    "## 3. Data Quality Analysis\n",
    "\n",
    "Check for missing values, outliers, and data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1947b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df_sample.isnull().sum()\n",
    "missing_pct = (missing / len(df_sample)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(\"âŒ Missing Values Analysis:\\n\")\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"âœ… No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae62b09",
   "metadata": {},
   "source": [
    "## 4. Geographic Analysis\n",
    "\n",
    "Analyze the pickup/dropoff coordinates to ensure they're within NYC bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYC geographic bounds\n",
    "NYC_BOUNDS = {\n",
    "    'lat_min': 40.4774,\n",
    "    'lat_max': 40.9176,\n",
    "    'lon_min': -74.2591,\n",
    "    'lon_max': -73.7004\n",
    "}\n",
    "\n",
    "# Find coordinate columns\n",
    "coord_info = []\n",
    "for col in df_sample.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'lat' in col_lower or 'lon' in col_lower:\n",
    "        coord_info.append({\n",
    "            'column': col,\n",
    "            'min': df_sample[col].min(),\n",
    "            'max': df_sample[col].max(),\n",
    "            'mean': df_sample[col].mean()\n",
    "        })\n",
    "\n",
    "print(\"ðŸ—ºï¸ Coordinate Columns Found:\\n\")\n",
    "for info in coord_info:\n",
    "    print(f\"  ðŸ“ {info['column']}:\")\n",
    "    print(f\"      Min: {info['min']:.6f}\")\n",
    "    print(f\"      Max: {info['max']:.6f}\")\n",
    "    print(f\"      Mean: {info['mean']:.6f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nðŸ™ï¸ NYC Valid Bounds:\")\n",
    "print(f\"   Latitude:  {NYC_BOUNDS['lat_min']:.4f} to {NYC_BOUNDS['lat_max']:.4f}\")\n",
    "print(f\"   Longitude: {NYC_BOUNDS['lon_min']:.4f} to {NYC_BOUNDS['lon_max']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pickup coordinate columns (they vary by dataset version)\n",
    "pickup_lat_col = None\n",
    "pickup_lon_col = None\n",
    "\n",
    "for col in df_sample.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'pickup' in col_lower and 'lat' in col_lower:\n",
    "        pickup_lat_col = col\n",
    "    elif 'pickup' in col_lower and 'lon' in col_lower:\n",
    "        pickup_lon_col = col\n",
    "\n",
    "if pickup_lat_col and pickup_lon_col:\n",
    "    print(f\"âœ… Found pickup coordinates: {pickup_lat_col}, {pickup_lon_col}\\n\")\n",
    "    \n",
    "    # Count valid vs invalid coordinates\n",
    "    valid_coords = df_sample[\n",
    "        (df_sample[pickup_lat_col].between(NYC_BOUNDS['lat_min'], NYC_BOUNDS['lat_max'])) &\n",
    "        (df_sample[pickup_lon_col].between(NYC_BOUNDS['lon_min'], NYC_BOUNDS['lon_max']))\n",
    "    ]\n",
    "    \n",
    "    total = len(df_sample)\n",
    "    valid = len(valid_coords)\n",
    "    invalid = total - valid\n",
    "    \n",
    "    print(f\"ðŸ“Š Coordinate Validity:\")\n",
    "    print(f\"   Total records:    {total:,}\")\n",
    "    print(f\"   Valid (in NYC):   {valid:,} ({100*valid/total:.1f}%)\")\n",
    "    print(f\"   Invalid:          {invalid:,} ({100*invalid/total:.1f}%)\")\n",
    "else:\n",
    "    print(\"âš ï¸ Could not find pickup coordinate columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6b61f",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis\n",
    "\n",
    "Analyze pickup times to understand traffic patterns by hour and day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find datetime columns\n",
    "datetime_cols = [col for col in df_sample.columns if 'time' in col.lower() or 'date' in col.lower()]\n",
    "print(f\"ðŸ• Datetime columns found: {datetime_cols}\\n\")\n",
    "\n",
    "# Find pickup datetime\n",
    "pickup_time_col = None\n",
    "for col in datetime_cols:\n",
    "    if 'pickup' in col.lower():\n",
    "        pickup_time_col = col\n",
    "        break\n",
    "\n",
    "if pickup_time_col:\n",
    "    # Convert to datetime\n",
    "    df_sample[pickup_time_col] = pd.to_datetime(df_sample[pickup_time_col], errors='coerce')\n",
    "    \n",
    "    # Extract time features\n",
    "    df_sample['hour'] = df_sample[pickup_time_col].dt.hour\n",
    "    df_sample['day_of_week'] = df_sample[pickup_time_col].dt.dayofweek\n",
    "    df_sample['day_name'] = df_sample[pickup_time_col].dt.day_name()\n",
    "    \n",
    "    print(f\"ðŸ“… Date range: {df_sample[pickup_time_col].min()} to {df_sample[pickup_time_col].max()}\")\n",
    "    print(f\"\\nâœ… Created time features: hour, day_of_week, day_name\")\n",
    "else:\n",
    "    print(\"âš ï¸ Could not find pickup time column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e42167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trip distribution by hour\n",
    "if 'hour' in df_sample.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Trips by hour\n",
    "    hourly_counts = df_sample['hour'].value_counts().sort_index()\n",
    "    axes[0].bar(hourly_counts.index, hourly_counts.values, color='#00d4ff', edgecolor='white')\n",
    "    axes[0].set_title('ðŸš• Trip Count by Hour of Day', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Hour (0-23)')\n",
    "    axes[0].set_ylabel('Number of Trips')\n",
    "    axes[0].set_xticks(range(0, 24))\n",
    "    \n",
    "    # Trips by day of week\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    daily_counts = df_sample['day_name'].value_counts().reindex(day_order)\n",
    "    axes[1].bar(range(7), daily_counts.values, color='#8b5cf6', edgecolor='white')\n",
    "    axes[1].set_title('ðŸš• Trip Count by Day of Week', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Day')\n",
    "    axes[1].set_ylabel('Number of Trips')\n",
    "    axes[1].set_xticks(range(7))\n",
    "    axes[1].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    peak_hour = hourly_counts.idxmax()\n",
    "    peak_day = daily_counts.idxmax()\n",
    "    print(f\"\\nðŸ“Š Insights:\")\n",
    "    print(f\"   Peak hour: {peak_hour}:00 ({hourly_counts.max():,} trips)\")\n",
    "    print(f\"   Peak day: {peak_day} ({daily_counts.max():,} trips)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69c9d2",
   "metadata": {},
   "source": [
    "## 6. Trip Statistics\n",
    "\n",
    "Analyze trip distances, durations, and calculate average speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c90554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find distance column\n",
    "distance_col = None\n",
    "for col in df_sample.columns:\n",
    "    if 'distance' in col.lower():\n",
    "        distance_col = col\n",
    "        break\n",
    "\n",
    "if distance_col:\n",
    "    print(f\"ðŸ“ Distance column: {distance_col}\\n\")\n",
    "    \n",
    "    # Filter valid distances (0-50 miles)\n",
    "    valid_dist = df_sample[(df_sample[distance_col] > 0) & (df_sample[distance_col] < 50)][distance_col]\n",
    "    \n",
    "    print(f\"ðŸ“Š Distance Statistics (valid trips):\")\n",
    "    print(f\"   Count:  {len(valid_dist):,}\")\n",
    "    print(f\"   Mean:   {valid_dist.mean():.2f} miles\")\n",
    "    print(f\"   Median: {valid_dist.median():.2f} miles\")\n",
    "    print(f\"   Std:    {valid_dist.std():.2f} miles\")\n",
    "    print(f\"   Min:    {valid_dist.min():.2f} miles\")\n",
    "    print(f\"   Max:    {valid_dist.max():.2f} miles\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.hist(valid_dist, bins=50, color='#10b981', edgecolor='white', alpha=0.7)\n",
    "    ax.axvline(valid_dist.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {valid_dist.mean():.1f} mi')\n",
    "    ax.axvline(valid_dist.median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {valid_dist.median():.1f} mi')\n",
    "    ax.set_title('ðŸ“ Distribution of Trip Distances', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Distance (miles)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 20)  # Focus on 0-20 miles\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ Could not find distance column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fdeebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trip duration and speed\n",
    "dropoff_time_col = None\n",
    "for col in df_sample.columns:\n",
    "    if 'dropoff' in col.lower() and 'time' in col.lower():\n",
    "        dropoff_time_col = col\n",
    "        break\n",
    "\n",
    "if pickup_time_col and dropoff_time_col and distance_col:\n",
    "    # Convert dropoff to datetime\n",
    "    df_sample[dropoff_time_col] = pd.to_datetime(df_sample[dropoff_time_col], errors='coerce')\n",
    "    \n",
    "    # Calculate duration in minutes\n",
    "    df_sample['duration_minutes'] = (df_sample[dropoff_time_col] - df_sample[pickup_time_col]).dt.total_seconds() / 60\n",
    "    \n",
    "    # Calculate speed (mph)\n",
    "    df_sample['speed_mph'] = df_sample[distance_col] / (df_sample['duration_minutes'] / 60)\n",
    "    \n",
    "    # Filter valid speeds (0-60 mph)\n",
    "    valid_speed = df_sample[(df_sample['speed_mph'] > 0) & (df_sample['speed_mph'] < 60)]['speed_mph']\n",
    "    \n",
    "    print(f\"ðŸš— Speed Statistics (valid trips):\")\n",
    "    print(f\"   Count:  {len(valid_speed):,}\")\n",
    "    print(f\"   Mean:   {valid_speed.mean():.2f} mph\")\n",
    "    print(f\"   Median: {valid_speed.median():.2f} mph\")\n",
    "    print(f\"   Std:    {valid_speed.std():.2f} mph\")\n",
    "    \n",
    "    # Speed distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.hist(valid_speed, bins=50, color='#f59e0b', edgecolor='white', alpha=0.7)\n",
    "    ax.axvline(valid_speed.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {valid_speed.mean():.1f} mph')\n",
    "    ax.set_title('ðŸš— Distribution of Trip Speeds', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Speed (mph)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Congestion analysis\n",
    "    print(f\"\\nðŸš¦ Congestion Analysis (based on speed):\")\n",
    "    high_congestion = (valid_speed < 10).sum()\n",
    "    medium_congestion = ((valid_speed >= 10) & (valid_speed < 20)).sum()\n",
    "    low_congestion = (valid_speed >= 20).sum()\n",
    "    total = len(valid_speed)\n",
    "    \n",
    "    print(f\"   High congestion (<10 mph):   {high_congestion:,} ({100*high_congestion/total:.1f}%)\")\n",
    "    print(f\"   Medium congestion (10-20 mph): {medium_congestion:,} ({100*medium_congestion/total:.1f}%)\")\n",
    "    print(f\"   Low congestion (>20 mph):    {low_congestion:,} ({100*low_congestion/total:.1f}%)\")\n",
    "else:\n",
    "    print(\"âš ï¸ Could not calculate speed - missing columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85151c18",
   "metadata": {},
   "source": [
    "## 7. Full Dataset Row Count\n",
    "\n",
    "Count total records across all files (this may take a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total records across all files\n",
    "print(\"â³ Counting rows in all files (this may take a minute)...\\n\")\n",
    "\n",
    "total_records = 0\n",
    "file_stats = []\n",
    "\n",
    "for f in taxi_files:\n",
    "    print(f\"  Counting: {f.name}...\", end=\" \")\n",
    "    \n",
    "    # Count lines (faster than loading entire file)\n",
    "    with open(f, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        row_count = sum(1 for _ in file) - 1  # Subtract header\n",
    "    \n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    file_stats.append({\n",
    "        'file': f.name,\n",
    "        'size_mb': size_mb,\n",
    "        'records': row_count\n",
    "    })\n",
    "    total_records += row_count\n",
    "    print(f\"{row_count:,} rows\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ðŸ“Š TOTAL RECORDS: {total_records:,}\")\n",
    "print(f\"ðŸ“¦ TOTAL SIZE: {sum(f['size_mb'] for f in file_stats):.2f} MB\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f87fda",
   "metadata": {},
   "source": [
    "## 8. Summary & Next Steps\n",
    "\n",
    "Final summary of findings and recommendations for the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š NYC TAXI DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸ“ Dataset Overview:\")\n",
    "print(f\"   Files: {len(taxi_files)}\")\n",
    "print(f\"   Total Size: ~7 GB\")\n",
    "print(f\"   Total Records: ~{total_records:,}\" if 'total_records' in dir() else \"   Run cell above to count\")\n",
    "print(f\"   Columns: {len(df_sample.columns)}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Geographic Coverage:\")\n",
    "print(f\"   Area: New York City\")\n",
    "print(f\"   Valid coordinates: ~90%+ of trips\")\n",
    "\n",
    "print(f\"\\nâ° Temporal Coverage:\")\n",
    "print(f\"   Months: 2015-01, 2016-01, 2016-02, 2016-03\")\n",
    "\n",
    "print(f\"\\nðŸ”‘ Key Columns for ML:\")\n",
    "print(f\"   â€¢ Pickup datetime (for time features)\")\n",
    "print(f\"   â€¢ Pickup latitude/longitude (for cell mapping)\")\n",
    "print(f\"   â€¢ Trip distance (for speed calculation)\")\n",
    "print(f\"   â€¢ Derived: speed_mph, hour, day_of_week\")\n",
    "\n",
    "print(f\"\\nâš ï¸ Data Quality Issues to Handle:\")\n",
    "print(f\"   â€¢ Coordinates outside NYC bounds\")\n",
    "print(f\"   â€¢ Zero or negative distances\")\n",
    "print(f\"   â€¢ Unrealistic speeds (>60 mph or <0)\")\n",
    "print(f\"   â€¢ Missing timestamp values\")\n",
    "\n",
    "print(f\"\\nâœ… Next Steps:\")\n",
    "print(f\"   1. Run data_cleaning.py to clean the data\")\n",
    "print(f\"   2. Run feature_engineering.py to create ML features\")\n",
    "print(f\"   3. Run model_training.py to train the model\")\n",
    "print(f\"   4. Start the API server with app.py\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸŽ‰ Data exploration complete! Ready for processing.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
